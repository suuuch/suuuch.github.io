---
title: 机器学习中的kNN近邻算法
layout: post
author: Suuuch
category: Machine Learing
tags:
- marchine learing
- Python
- kNN
- 算法
---
# 算法简介

最近邻居法（KNN算法，又译K-近邻算法）是一种用于分类和回归的非参数统计方法。

- 在k-NN分类中，输出是一个分类族群。一个对象的分类是由其邻居的“多数表决”确定的，k个最近邻居（k为正整数，通常较小）中最常见的分类决定了赋予该对象的类别。若k = 1，则该对象的类别直接由最近的一个节点赋予。

- 在k-NN回归中，输出是该对象的属性值。该值是其k个最近邻居的值的平均值。

最近邻居法采用向量空间模型来分类，概念为相同类别的案例，彼此的相似度高，而可以借由计算与已知类别案例之相似度，来评估未知类别案例可能的分类。

# 算法流程

1. 将现有数据使用向量表示
2. 计算两个向量之间的距离
3. 判断新输入向量特定范围内的包含的已分类的向量
4. 统计在范围阈值内的分类数量
5. 输入向量通过“多数表决”，来判断属于哪个分类

# 优缺点

1. 优点：
	算法简单，易于实现，不需要参数估计，不需要事先训练。
2. 缺点：
	属于懒惰算法，“平时不好好学习，考试时才临阵磨枪”，意思是kNN不用事先训练，而是在输入待分类样本时才开始运行，这一特点导致kNN计算量特别大，而且训练样本必须存储在本地，内存开销也特别大。

# 运算逻辑示例

##  以二维向量为例：

存在数据：





转换为向量

例子中的是二维数组，转换为矩阵

$$\begin{bmatrix}x_{11} & x_{12} & x_{13}\\x_{21} &x_{22} &x_{23}\\x_{31} &x_{32} &x_{33} \end{bmatrix}$$ 

> 数据中所有非数值型数据全部都需要映射为特定的数字

分解矩阵，在矩阵中的$$x_{n1}$$ $$x_{n2}$$ 为因变量 $$x_{n3}$$为解释变量

输入向量$$\begin{bmatrix}x_i & x_{1j}\end{bmatrix}$$
开始计算当前向量与已存在的矩阵中所有向量的距离，
距离计算公式：
欧式距离：
在欧几里得空间中，点$$x =(x_1,...,x_n)$$和 $$y =(y_1,...,y_n)$$之间的欧氏距离为:
$$d_{12}=\sqrt{ {(x_1 - y_1)}^2 + {(x_2 - y_2)}^2 }$$

